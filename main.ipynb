{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Loading-the-dataset\" data-toc-modified-id=\"Loading-the-dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Loading the dataset</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets used in this notebook can be found <a href=\"https://github.com/susanli2016/NLP-with-Python/tree/master/data\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "\n",
    "data_dir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.0.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('2.0'), 'Please use TensorFlow version 2.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'small_vocab_en'), 'r') as f:\n",
    "    en_corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'small_vocab_fr'), 'r') as f:\n",
    "    fr_corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-English sentence: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "-French translation: new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "-English sentence: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "-French translation: les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n",
      "-English sentence: california is usually quiet during march , and it is usually hot in june .\n",
      "-French translation: california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "\n",
      "-English sentence: the united states is sometimes mild during june , and it is cold in september .\n",
      "-French translation: les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "\n",
      "-English sentence: your least liked fruit is the grape , but my least liked is the apple .\n",
      "-French translation: votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('-English sentence: ', end='')\n",
    "    print(en_corpus.split('\\n')[i])\n",
    "    print('-French translation: ', end='')\n",
    "    print(fr_corpus.split('\\n')[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentences = en_corpus.split('\\n')\n",
    "fr_sentences = fr_corpus.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding <EOS> to the end of each sentence\n",
    "en_sentences = [stn.split()+['<EOS>'] for stn in en_sentences]\n",
    "fr_sentences = [stn.split()+['<EOS>'] for stn in fr_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = set()\n",
    "for stn in en_sentences:\n",
    "    for word in stn:\n",
    "        en_vocab.add(word)\n",
    "        \n",
    "        \n",
    "fr_vocab = set()\n",
    "for stn in fr_sentences:\n",
    "    for word in stn:\n",
    "        fr_vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 228 words in the english vocab\n",
      "There are 356 word in the french vocab\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} words in the english vocab\".format(len(en_vocab)))\n",
    "print(\"There are {} word in the french vocab\".format(len(fr_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "en_vocab_to_int = {w: i for i,w in enumerate(en_vocab)}\n",
    "en_int_to_vocab = dict(enumerate(en_vocab))\n",
    "fr_vocab_to_int = {w: i for i,w in enumerate(fr_vocab)}\n",
    "fr_int_to_vocab = dict(enumerate(fr_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_encoded = [[en_vocab_to_int[word] for word in stn] for stn in en_sentences]\n",
    "fr_encoded = [[fr_vocab_to_int[word] for word in stn] for stn in fr_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_padded = tf.keras.preprocessing.sequence.pad_sequences(en_encoded, padding='post')\n",
    "fr_padded = tf.keras.preprocessing.sequence.pad_sequences(fr_encoded, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(en_padded)\n",
    "y = np.array(fr_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, Tx):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        self.repeat = layers.RepeatVector(Tx)\n",
    "        self.concat = layers.Concatenate(axis=-1)\n",
    "        self.fc1 = layers.Dense(50, activation='relu')\n",
    "        self.fc2 = layers.Dense(1, activation='tanh')\n",
    "        self.activate = layers.Softmax(axis=1)\n",
    "        self.dotor = layers.Dot(axes=1)\n",
    "        \n",
    "    def call(self, a, s):\n",
    "        '''\n",
    "        Returns the context using the lstm previous hidden state (s) and the bidirectional lstm output (a)\n",
    "        '''\n",
    "        s = self.repeat(s)\n",
    "        conc = self.concat([a, s])\n",
    "        x = self.fc2(self.fc1(conc))\n",
    "        alphas = self.activate(x)\n",
    "        context = self.dotor([alphas, a])\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(tf.keras.models.Model):\n",
    "    def __init__(self, Tx, Ty, input_size, output_size, att_size, hidden_size, embed_size):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.Ty = Ty\n",
    "        \n",
    "        self.embed = layers.Embedding(input_size, embed_size)\n",
    "        \n",
    "        self.att_layer = AttentionLayer(Tx)\n",
    "        \n",
    "        self.bidir = layers.Bidirectional(layers.LSTM(att_size, return_sequences=True))\n",
    "        \n",
    "        self.post_activation = layers.LSTM(hidden_size, return_state=True)\n",
    "        \n",
    "        self.fc = layers.Dense(output_size, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        s = tf.zeros(shape=(x.shape[0],self.hidden_size))\n",
    "        c = tf.zeros(shape=(x.shape[0],self.hidden_size))\n",
    "        \n",
    "        a = self.bidir(self.embed(x))\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(Ty):\n",
    "            \n",
    "            context = self.att_layer(a, s)\n",
    "            \n",
    "            s, hid, c = self.post_activation(context, initial_state=[s,c])\n",
    "            \n",
    "            out = self.fc(hid)\n",
    "            \n",
    "            outputs.append(out)\n",
    "            \n",
    "        return tf.transpose(tf.stack(outputs), [1,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_size = 64\n",
    "hidden_size = 128\n",
    "Tx = X.shape[1]\n",
    "Ty = y.shape[1]\n",
    "input_size = len(en_int_to_vocab)\n",
    "output_size = len(fr_int_to_vocab)\n",
    "embed_size = 100\n",
    "epochs = 30\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionModel(Tx, Ty, input_size, output_size, att_size, hidden_size, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        yield X[i:i+batch_size], y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  70,  77,  97, 190,   0,  58,  68, 139, 222,  77,  48,  78,\n",
       "       227, 158,  81,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(X, y, model):\n",
    "    \n",
    "    idx = np.random.choice(np.arange(len(X)), size=1)[0]\n",
    "    output = tf.argmax(model(X[idx:idx+1]), axis=-1).numpy()\n",
    "    output = np.reshape(output, -1).tolist()\n",
    "    en_stn = []\n",
    "    fr_stn = []\n",
    "    model_stn = []\n",
    "    \n",
    "    for i in X[idx]:\n",
    "        if en_int_to_vocab[i]=='<EOS>':\n",
    "            break\n",
    "        en_stn.append(en_int_to_vocab[i])\n",
    "        \n",
    "    for i in y[idx]:\n",
    "        if fr_int_to_vocab[i]=='<EOS>':\n",
    "            break\n",
    "        fr_stn.append(fr_int_to_vocab[i])\n",
    "        \n",
    "    for i in output:\n",
    "        if fr_int_to_vocab[i]=='<EOS>':\n",
    "            break\n",
    "        model_stn.append(fr_int_to_vocab[i])\n",
    "      \n",
    "    print()\n",
    "    print('-English sentence: ', end='')\n",
    "    print(' '.join(en_stn))\n",
    "    print('-French translation: ', end='')\n",
    "    print(' '.join(fr_stn))\n",
    "    print('-Model translation: ', end='')\n",
    "    print(' '.join(model_stn))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from ./tf_ckpts/ckpt-13\n",
      "Epoch: 1/30 ... Avg. Loss: 0.021463120356202126\n",
      "Saved checkpoint for step 1400: ./tf_ckpts/ckpt-14\n",
      "Epoch: 1/30 ... Avg. Loss: 0.020839454606175423\n",
      "Saved checkpoint for step 1500: ./tf_ckpts/ckpt-15\n",
      "Epoch: 1/30 ... Avg. Loss: 0.020445620641112328\n",
      "Saved checkpoint for step 1600: ./tf_ckpts/ckpt-16\n",
      "Epoch: 1/30 ... Avg. Loss: 0.019953647628426552\n",
      "Saved checkpoint for step 1700: ./tf_ckpts/ckpt-17\n",
      "Epoch: 1/30 ... Avg. Loss: 0.019731489941477776\n",
      "Saved checkpoint for step 1800: ./tf_ckpts/ckpt-18\n",
      "Epoch: 1/30 ... Avg. Loss: 0.01939842291176319\n",
      "Saved checkpoint for step 1900: ./tf_ckpts/ckpt-19\n",
      "Epoch: 1/30 ... Avg. Loss: 0.018929995596408844\n",
      "Saved checkpoint for step 2000: ./tf_ckpts/ckpt-20\n",
      "\n",
      "-English sentence: california is sometimes dry during summer , but it is never warm in february .\n",
      "-French translation: californie est parfois sec pendant l' été , mais il est jamais chaud en février .\n",
      "-Model translation: paris est jamais agréable au mois de , mais il est est parfois en en .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "    \n",
    "    \n",
    "for e in range(epochs):\n",
    "    for inputs, targets in get_batches(X, y, batch_size):\n",
    "        \n",
    "        ckpt.step.assign_add(1)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_object(targets, outputs)\n",
    "        \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        losses.append(loss/inputs.shape[0])\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        if int(ckpt.step)%100==0:\n",
    "            print(\"Epoch: {}/{} ... Avg. Loss: {}\".format(e+1, epochs, np.mean(losses[-100:])))\n",
    "                  \n",
    "            save_path = manager.save()\n",
    "            print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "            \n",
    "        if int(ckpt.step)%1000==0:\n",
    "            sample_from_model(X, y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
